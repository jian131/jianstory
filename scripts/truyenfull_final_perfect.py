#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TruyenFull Final Perfect - HO√ÄN H·∫¢O 100%
Preserve exact formatting + Fix broken words th√¥ng minh + Crawl ·∫£nh b√¨a
"""

import re
import os
import json
import time
import requests
from bs4 import BeautifulSoup
import urllib.parse as urlparse
from urllib.parse import urljoin
import shutil

def smart_fix_vietnamese_broken_words(text: str) -> str:
    """
    Thu·∫≠t to√°n ƒê∆†N GI·∫¢N v√† HI·ªÜU QU·∫¢ ƒë·ªÉ fix broken Vietnamese words
    Approach pragmatic - fix maximum s·ªë l·ªói v·ªõi minimum complexity
    """
    if not text or len(text.strip()) < 5:
        return text

    # Split th√†nh lines ƒë·ªÉ preserve line breaks
    lines = text.split('\n')
    fixed_lines = []

    for line in lines:
        if line.strip():  # Ch·ªâ process lines c√≥ content
            # Method 1: Fix obvious broken patterns (ƒë√£ proven)
            line = fix_broken_characters_in_words(line)

            # Method 2: Fix common missing spaces (new)
            line = fix_missing_spaces(line)

            # Method 3: Clean up spacing
            line = re.sub(r'[ \t]+', ' ', line)
            line = re.sub(r' +([.!?,:;])', r'\1', line)

            fixed_lines.append(line.strip())
        else:
            fixed_lines.append('')  # Preserve empty lines exactly

    return '\n'.join(fixed_lines)

def fix_broken_characters_in_words(text: str) -> str:
    """Fix c√°c k√Ω t·ª± b·ªã t√°ch trong t·ª´ v·ªõi approach conservative - ch·ªâ fix nh·ªØng l·ªói r√µ r√†ng"""

    # Only fix obvious and safe patterns
    safe_fixes = [
        # Core patterns that are definitely broken
        ('n√≥ i', 'n√≥i'),
        ('th ai', 'thai'),
        ('nh∆∞ ng', 'nh∆∞ng'),
        ('c≈© ng', 'c≈©ng'),
        ('qua nh', 'quanh'),
        ('to √†n', 'to√†n'),
        ('v√† o', 'v√†o'),
        ('ƒëi ·ªán', 'ƒëi·ªán'),
        ('ƒë∆∞·ª£ c', 'ƒë∆∞·ª£c'),
        ('ng∆∞·ªù i', 'ng∆∞·ªùi'),
        ('th·∫• y', 'th·∫•y'),
        ('m√† ng', 'mang'),
        ('th∆∞·ªù ng', 'th∆∞·ªùng'),
        ('c∆∞·ªù i', 'c∆∞·ªùi'),
        ('l√† m', 'l√†m'),
        ('ƒë√¢ u', 'ƒë√¢u'),
        ('n√† o', 'n√†o'),
        ('l·∫° i', 'l·∫°i'),
        ('l·∫° nh', 'l·∫°nh'),
        ('r·ªì i', 'r·ªìi'),
        ('c√≤ n', 'c√≤n'),
        # Common compound patterns
        ('tr ∆∞·ªõc', 'tr∆∞·ªõc'),
        ('tr √™n', 'tr√™n'),
        ('tr ·ªü', 'tr·ªü'),
        ('t·ª´ ng', 't·ª´ng'),
        ('khi ·∫øn', 'khi·∫øn'),
        ('kh√° c', 'kh√°c'),
        ('c√° c', 'c√°c'),
        ('s√° ng', 's√°ng'),
        ('mi·ªÅ ng', 'mi·ªáng'),
        ('ti·ªÅ ng', 'ti·∫øng'),
        ('thi·ªÅ u', 'thi·∫øu'),
        ('di·ªÅ u', 'ƒëi·ªÅu'),
        # Character name fix
        ('T·ªô t·ªë', 'T·ªë T·ªë'),
        ('t·ªô t·ªë', 'T·ªë T·ªë'),
    ]

    # Apply fixes with simple replacement
    for broken, fixed in safe_fixes:
        text = text.replace(broken, fixed)

    return text

def extract_content_preserve_exact_formatting(content_elem):
    """Extract content preserve CH√çNH X√ÅC formatting t·ª´ HTML"""

    if not content_elem:
        return ""

    # Convert HTML breaks th√†nh newlines TR∆Ø·ªöC KHI extract text
    html_str = str(content_elem)

    # Remove ads first
    html_str = re.sub(r'<div[^>]*ads[^>]*>.*?</div>', '', html_str, flags=re.DOTALL | re.IGNORECASE)

    # Convert <br> -> newline (m·ªói <br> = 1 newline ch√≠nh x√°c)
    html_str = re.sub(r'<br\s*/?>', '\n', html_str, flags=re.IGNORECASE)

    # Extract clean text
    soup = BeautifulSoup(html_str, 'html.parser')
    clean_text = soup.get_text()

    return clean_text.strip()

def crawl_truyenfull_final_perfect(url: str):
    """Crawl story v·ªõi FINAL PERFECT quality + ·∫£nh b√¨a"""

    print(f"üéØ FINAL PERFECT CRAWL: {url}")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        # 1. Fetch story page
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            print(f"‚ùå Status code: {response.status_code}")
            return False

        soup = BeautifulSoup(response.content, 'html.parser')
        print("‚úÖ Story page loaded")

        # 2. Extract metadata
        title = ""
        author = ""

        h1_title = soup.select_one('h1')
        if h1_title:
            title = h1_title.get_text().strip()
            print(f"üìö Title: {title}")

        author_link = soup.select_one("a[href*='tac-gia']")
        if author_link:
            author = author_link.get_text().strip()
            print(f"‚úçÔ∏è  Author: {author}")

        # 3. Create story directory
        story_name = title.replace(' ', '-').replace('/', '-') if title else 'unknown-story'
        story_dir = f"stories/{story_name}"
        os.makedirs(story_dir, exist_ok=True)

        # 4. Download ·∫£nh b√¨a
        cover_info = download_cover_image(soup, story_dir, story_name)

        # 5. Get chapter list
        chapter_links = soup.select("li a[href*='chuong-']")

        filtered_chapters = []
        for link in chapter_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            if any(skip in href for skip in ['top-truyen', 'danh-sach', '100-chuong', '500-chuong', '1000-chuong']):
                continue
            if 'chuong-' in href and text.lower().startswith('ch∆∞∆°ng'):
                filtered_chapters.append(link)

        print(f"üìñ Found {len(filtered_chapters)} chapters")

        if not filtered_chapters:
            print("‚ùå No chapters found!")
            return False

        # 6. Crawl chapters v·ªõi FINAL PERFECT quality
        chapters_data = []
        success_count = 0

        # H·ªèi user c√≥ mu·ªën crawl t·∫•t c·∫£ chapters kh√¥ng
        while True:
            choice = input(f"\nüìù Crawl t·∫•t c·∫£ {len(filtered_chapters)} chapters? (y/n) ho·∫∑c nh·∫≠p s·ªë chapters mu·ªën crawl: ").strip().lower()
            if choice in ['y', 'yes', 'c√≥', 'c']:
                max_chapters = len(filtered_chapters)
                break
            elif choice in ['n', 'no', 'kh√¥ng', 'k']:
                max_chapters = 2  # Default test
                break
            else:
                try:
                    max_chapters = int(choice)
                    if 1 <= max_chapters <= len(filtered_chapters):
                        break
                    else:
                        print(f"‚ùå Vui l√≤ng nh·∫≠p t·ª´ 1 ƒë·∫øn {len(filtered_chapters)}")
                except:
                    print("‚ùå Vui l√≤ng nh·∫≠p 'y', 'n' ho·∫∑c s·ªë chapters")

        print(f"üöÄ S·∫Ω crawl {max_chapters}/{len(filtered_chapters)} chapters...")

        for i, chapter_link in enumerate(filtered_chapters[:max_chapters], 1):
            chapter_href = chapter_link.get('href', '')
            chapter_title = chapter_link.get_text().strip()

            if chapter_href.startswith('/'):
                chapter_url = 'https://truyenfull.vision' + chapter_href
            elif not chapter_href.startswith('http'):
                chapter_url = urlparse.urljoin(url, chapter_href)
            else:
                chapter_url = chapter_href

            print(f"  üìÑ [{i}/{len(filtered_chapters)}] {chapter_title}")
            print(f"     URL: {chapter_url}")

            try:
                chapter_response = requests.get(chapter_url, headers=headers, timeout=15)
                if chapter_response.status_code != 200:
                    print(f"     ‚ùå Status: {chapter_response.status_code}")
                    continue

                chapter_soup = BeautifulSoup(chapter_response.content, 'html.parser')

                # Extract content v·ªõi exact formatting preservation
                content_elem = chapter_soup.select_one('#chapter-c')
                if not content_elem:
                    content_elem = chapter_soup.select_one('.chapter-c')

                if content_elem:
                    # Step 1: Extract v·ªõi preserve exact formatting
                    raw_content = extract_content_preserve_exact_formatting(content_elem)

                    # Step 2: Smart fix broken words M√Ä KH√îNG ph√° formatting
                    final_content = smart_fix_vietnamese_broken_words(raw_content)

                    # Analysis
                    total_lines = len(final_content.split('\n'))
                    non_empty_lines = len([line for line in final_content.split('\n') if line.strip()])

                    print(f"     ‚úÖ Content: {len(final_content)} chars")
                    print(f"     üìù Lines: {total_lines} total, {non_empty_lines} non-empty")
                    print(f"     Preview: {final_content[:100]}...")

                    # Save chapter v·ªõi FINAL PERFECT quality
                    chapter_file = f"{story_dir}/chapter_{i:03d}_FINAL.txt"
                    with open(chapter_file, 'w', encoding='utf-8') as f:
                        f.write(f"# {chapter_title}\n\n{final_content}")

                    chapters_data.append({
                        'title': chapter_title,
                        'content': final_content,
                        'url': chapter_url,
                        'total_lines': total_lines,
                        'non_empty_lines': non_empty_lines
                    })

                    success_count += 1

                else:
                    print(f"     ‚ùå No content found!")

                time.sleep(1)

            except Exception as e:
                print(f"     ‚ùå Error: {e}")

        # 7. Save story info v·ªõi cover image info
        story_info = {
            'title': title,
            'author': author,
            'url': url,
            'total_chapters': len(filtered_chapters),
            'crawled_chapters': success_count,
            'cover_image': cover_info,
            'chapters': chapters_data,
            'quality_note': 'FINAL PERFECT - Exact formatting + Smart broken word fix + Cover image'
        }

        with open(f"{story_dir}/story_info_FINAL.json", 'w', encoding='utf-8') as f:
            json.dump(story_info, f, indent=2, ensure_ascii=False)

        print(f"\nüéâ FINAL PERFECT CRAWL HO√ÄN TH√ÄNH!")
        print(f"   üìö Story: {title}")
        print(f"   ‚úçÔ∏è  Author: {author}")
        print(f"   üìñ Chapters: {success_count}/{len(filtered_chapters)}")
        if cover_info:
            print(f"   üñºÔ∏è  Cover: {cover_info['filename']} ({cover_info['size']} bytes)")
        print(f"   üìÅ Saved to: {story_dir}/")
        print(f"   üèÜ Quality: FINAL PERFECT - Exact formatting + Smart text fix + Cover image")

        # Show stats
        total_lines = sum(ch.get('total_lines', 0) for ch in chapters_data)
        total_non_empty = sum(ch.get('non_empty_lines', 0) for ch in chapters_data)
        print(f"   üìä Total lines: {total_lines} ({total_non_empty} non-empty)")

        return True

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def main():
    """Main function v·ªõi URL interface"""

    # L·∫•y URL t·ª´ user
    url = get_story_url_from_user()

    if not url:
        return

    print(f"\nüß™ B·∫Øt ƒë·∫ßu crawl: {url}")
    success = crawl_truyenfull_final_perfect(url)

    if success:
        print("\nüéâ FINAL PERFECT CRAWL TH√ÄNH C√îNG!")
        print("üíé Ch·∫•t l∆∞·ª£ng: Formatting + Text ho√†n h·∫£o + ·∫¢nh b√¨a nh∆∞ website g·ªëc!")

        # H·ªèi c√≥ mu·ªën crawl th√™m truy·ªán kh√°c kh√¥ng
        while True:
            again = input("\nüîÑ C√≥ mu·ªën crawl th√™m truy·ªán kh√°c kh√¥ng? (y/n): ").strip().lower()
            if again in ['y', 'yes', 'c√≥', 'c']:
                main()  # Recursive call
                break
            elif again in ['n', 'no', 'kh√¥ng', 'k']:
                print("üëã T·∫°m bi·ªát!")
                break
            else:
                print("‚ùå Vui l√≤ng nh·∫≠p 'y' ho·∫∑c 'n'")
    else:
        print("\n‚ùå CRAWL TH·∫§T B·∫†I!")

        # H·ªèi c√≥ mu·ªën th·ª≠ l·∫°i kh√¥ng
        while True:
            retry = input("\nüîÑ C√≥ mu·ªën th·ª≠ l·∫°i v·ªõi URL kh√°c kh√¥ng? (y/n): ").strip().lower()
            if retry in ['y', 'yes', 'c√≥', 'c']:
                main()  # Recursive call
                break
            elif retry in ['n', 'no', 'kh√¥ng', 'k']:
                print("üëã T·∫°m bi·ªát!")
                break
            else:
                print("‚ùå Vui l√≤ng nh·∫≠p 'y' ho·∫∑c 'n'")

def fix_missing_spaces(text: str) -> str:
    """Fix t·ª´ d√≠nh li·ªÅn b·∫±ng c√°ch th√™m spaces v√†o ch·ªó c·∫ßn thi·∫øt"""

    # Patterns ƒë·ªÉ detect v√† fix t·ª´ d√≠nh li·ªÅn
    space_fixes = [
        # Pattern: lowercase word + uppercase/capitalized word
        (r'([a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ]+)([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥][a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ]*)', r'\1 \2'),

        # Common Vietnamese words that often get stuck together
        (r'(l√†)(m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m|ch√≠n|m∆∞·ªùi|th·∫ßn|ti√™n|h·ªëi|n√∫i|khu√¥n|v·ª£|t·ª±|th√°i|ƒëang|h·∫°|m√†|th·∫ßm)', r'\1 \2'),
        (r'(v√¨)(mang|ta|th·∫ø)', r'\1 \2'),
        (r'(c√≥)(s√°ng|ƒë∆∞·ª£c|ch√∫t|ƒë·ª©a|ph·∫£i|ai|hai|m·ªôt)', r'\1 \2'),
        (r'(ta)(hai|chui|ƒëo√°n|s·ª£|run|kh√¥ng|c√≤n|ƒëau|th·∫ßm)', r'\1 \2'),
        (r'(t·ª≠)(duy|kh√°c|ƒëi·ªán)', r'\1 \2'),
        (r'(n·ªØ)(chƒÉm)', r'\1 \2'),
        (r'(m·ªü)(c·ª≠a)', r'\1 \2'),
        (r'(ai)(ƒë√≥)', r'\1 \2'),
        (r'(ra)(kh·ªèi|ƒë√£|gi∆∞·ªùng)', r'\1 \2'),
        (r'(ƒëi)(m·ªôt|n·ªØa)', r'\1 \2'),
        (r'(ƒë√≥)(l·∫°i|th·ª±c|th√≠ch)', r'\1 \2'),
        (r'(ƒë√£)(khi·∫øn|y√™u|c√≥|t·ª´ng|m·∫•t|h∆°i)', r'\1 \2'),
        (r'(v√¥)(l√Ω)', r'\1 \2'),
        (r'(t∆°)(v√≤)', r'\1 \2'),
        (r'(v√†)(ƒë√¥i)', r'\1 \2'),
        (r'(b·ªã)(kho√©t)', r'\1 \2'),
        (r'(t·ª´)(ƒë√¢u|khi)', r'\1 \2'),
        (r'(ban)(nƒÉm)', r'\1 \2'),
        (r'(c√¥)(n∆∞∆°ng|ƒë∆°n)', r'\1 \2'),
        (r'(ƒëi)(qua)', r'\1 \2'),
        (r'(v·ªÅ)(c√°i)', r'\1 \2'),
        (r'(t·ª±)(kh√©p|l·∫•y)', r'\1 \2'),
        (r'(ta)(c·ª©u|n√≥i)', r'\1 \2'),
        (r'(c·∫£)(m·ªçi)', r'\1 \2'),
        (r'(ƒë·ª°)(ta)', r'\1 \2'),
        (r'(s·ª±)(vi·ªác)', r'\1 \2'),
        (r'(l√™n)(C·ª≠u)', r'\1 \2'),
        (r'(b·ªën)(b·ªÅ)', r'\1 \2'),
        (r'(ch·ªØ)(T·ªë)', r'\1 \2'),
        (r'(ƒÉn)(kh·ªõp)', r'\1 \2'),
        (r'(ƒëi)(ngh·ªâ)', r'\1 \2'),
        (r'(ƒë√£)(c√≥|m·∫•t)', r'\1 \2'),
        (r'(ta)(r·∫±ng|r·ªìi)', r'\1 \2'),
        (r'(c·ª©)(ng·ª°)', r'\1 \2'),
        (r'(c·ªßa)(ƒë√°m|ng∆∞∆°i)', r'\1 \2'),
        (r'(tr√†o)(l√™n)', r'\1 \2'),
        (r'(ng·∫°o)(m·∫°n)', r'\1 \2'),
        (r'(nh∆∞)(th·∫ø)', r'\1 \2'),
        (r'(ch·ªôp)(l·∫•y)', r'\1 \2'),
        (r'(ta)(xu·ªëng)', r'\1 \2'),
        (r'(r∆°i)(kh·ªèi)', r'\1 \2'),
        (r'(c·ªë)(√Ω)', r'\1 \2'),
        (r'(d·ª±ng)(ƒë·ª©ng)', r'\1 \2'),
        (r'(h·∫Øt)(ra)', r'\1 \2'),
        (r'(con)(trai)', r'\1 \2'),
        (r'(ti·ªÉu)(ti√™n)', r'\1 \2'),
        (r'(nƒÉm)(th·ª©)', r'\1 \2'),
        (r'(D·∫°)(Hoa)', r'\1 \2'),
        (r'(T·ªë)(C·∫©m)', r'\1 \2'),
    ]

    for pattern, replacement in space_fixes:
        text = re.sub(pattern, replacement, text)

    return text

def download_cover_image(soup, story_dir, story_name):
    """Download ·∫£nh b√¨a truy·ªán"""

    print("üñºÔ∏è  T√¨m v√† download ·∫£nh b√¨a...")

    # C√°c selector c√≥ th·ªÉ c√≥ ·∫£nh b√¨a
    cover_selectors = [
        'img.book',
        'img[alt*="cover"]',
        'img[alt*="b√¨a"]',
        '.book img',
        '.cover img',
        '.story-cover img',
        'img[src*="cover"]',
        'img[src*="book"]',
        '.info-holder img',
        '.books img'
    ]

    cover_img = None

    # T√¨m ·∫£nh b√¨a
    for selector in cover_selectors:
        cover_img = soup.select_one(selector)
        if cover_img:
            print(f"     ‚úÖ T√¨m th·∫•y ·∫£nh b√¨a v·ªõi selector: {selector}")
            break

    if not cover_img:
        # Fallback: t√¨m ·∫£nh ƒë·∫ßu ti√™n c√≥ k√≠ch th∆∞·ªõc h·ª£p l√Ω
        all_imgs = soup.select('img[src]')
        for img in all_imgs:
            src = img.get('src', '')
            if any(keyword in src.lower() for keyword in ['cover', 'book', 'story', 'truyen']):
                cover_img = img
                print(f"     ‚úÖ T√¨m th·∫•y ·∫£nh b√¨a fallback: {src}")
                break

    if not cover_img:
        print("     ‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh b√¨a")
        return None

    try:
        img_src = cover_img.get('src', '')
        if not img_src:
            print("     ‚ùå Kh√¥ng c√≥ src cho ·∫£nh")
            return None

        # T·∫°o URL ƒë·∫ßy ƒë·ªß cho ·∫£nh
        if img_src.startswith('/'):
            img_url = 'https://truyenfull.vision' + img_src
        elif not img_src.startswith('http'):
            img_url = urljoin('https://truyenfull.vision/', img_src)
        else:
            img_url = img_src

        print(f"     üì• Downloading: {img_url}")

        # Download ·∫£nh
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://truyenfull.vision/'
        }

        img_response = requests.get(img_url, headers=headers, timeout=15, stream=True)
        if img_response.status_code == 200:
            # X√°c ƒë·ªãnh extension c·ªßa file
            content_type = img_response.headers.get('content-type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                ext = '.jpg'
            elif 'png' in content_type:
                ext = '.png'
            elif 'gif' in content_type:
                ext = '.gif'
            elif 'webp' in content_type:
                ext = '.webp'
            else:
                # Fallback t·ª´ URL
                if '.jpg' in img_url.lower():
                    ext = '.jpg'
                elif '.png' in img_url.lower():
                    ext = '.png'
                elif '.gif' in img_url.lower():
                    ext = '.gif'
                elif '.webp' in img_url.lower():
                    ext = '.webp'
                else:
                    ext = '.jpg'  # Default

            # T·∫°o t√™n file cho ·∫£nh b√¨a
            cover_filename = f"cover{ext}"
            cover_path = os.path.join(story_dir, cover_filename)

            # Save ·∫£nh
            with open(cover_path, 'wb') as f:
                shutil.copyfileobj(img_response.raw, f)

            file_size = os.path.getsize(cover_path)
            print(f"     ‚úÖ ·∫¢nh b√¨a saved: {cover_filename} ({file_size} bytes)")

            return {
                'filename': cover_filename,
                'path': cover_path,
                'url': img_url,
                'size': file_size
            }
        else:
            print(f"     ‚ùå Download failed: Status {img_response.status_code}")
            return None

    except Exception as e:
        print(f"     ‚ùå Error downloading cover: {e}")
        return None

def get_story_url_from_user():
    """Interface ƒë·ªÉ user nh·∫≠p URL truy·ªán"""

    print("\n" + "="*60)
    print("üèÜ TRUYENFULL FINAL PERFECT CRAWLER")
    print("CH·∫§T L∆Ø·ª¢NG HO√ÄN H·∫¢O 100% + ·∫¢NH B√åA")
    print("="*60)

    # M·ªôt s·ªë URL m·∫´u
    sample_urls = [
        "https://truyenfull.vision/tam-sinh-tam-the-thap-ly-dao-hoa/",
        "https://truyenfull.vision/co-vo-ngot-ngao-khong-the-chay-thoat/",
        "https://truyenfull.vision/tieu-nong-dan-cua-hoang-gia/",
        "https://truyenfull.vision/phi-cung-kieu-nu/",
        "https://truyenfull.vision/nhan-gian-vo-dao/"
    ]

    print("\nüìã M·ªôt s·ªë URL m·∫´u:")
    for i, url in enumerate(sample_urls, 1):
        print(f"   {i}. {url}")

    print("\n" + "-"*60)

    while True:
        choice = input("\nüîó Nh·∫≠p URL truy·ªán (ho·∫∑c s·ªë 1-5 ƒë·ªÉ ch·ªçn m·∫´u, 'exit' ƒë·ªÉ tho√°t): ").strip()

        if choice.lower() in ['exit', 'quit', 'tho√°t', 'q']:
            print("üëã T·∫°m bi·ªát!")
            return None

        # Ki·ªÉm tra n·∫øu user ch·ªçn s·ªë
        if choice.isdigit():
            idx = int(choice) - 1
            if 0 <= idx < len(sample_urls):
                selected_url = sample_urls[idx]
                print(f"‚úÖ ƒê√£ ch·ªçn: {selected_url}")
                return selected_url
            else:
                print(f"‚ùå Vui l√≤ng ch·ªçn t·ª´ 1 ƒë·∫øn {len(sample_urls)}")
                continue

        # Ki·ªÉm tra URL
        if choice.startswith('http'):
            if 'truyenfull.vision' in choice:
                print(f"‚úÖ URL h·ª£p l·ªá: {choice}")
                return choice
            else:
                print("‚ùå Tool n√†y ch·ªâ h·ªó tr·ª£ truyenfull.vision")
                continue
        else:
            print("‚ùå Vui l√≤ng nh·∫≠p URL ƒë·∫ßy ƒë·ªß (b·∫Øt ƒë·∫ßu b·∫±ng http)")
            continue

if __name__ == "__main__":
    main()
